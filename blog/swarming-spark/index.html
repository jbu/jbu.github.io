<!doctype html><html lang=en-us><head><title>Swarming Spark | James Uther</title><meta charset=utf-8><meta name=language content="en"><meta name=description content><meta name=keywords content><meta name=viewport content="width=device-width,initial-scale=1"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=twitter:card content="summary"><meta name=twitter:title content="Swarming Spark"><meta name=twitter:description content><meta name=twitter:site content="https://twitter.com/hemul"><meta name=twitter:creator content="https://twitter.com/hemul"><link rel="shortcut icon" type=image/png href=https://uther.wtf/favicon.ico><link type=text/css rel=stylesheet href=https://uther.wtf/css/post.min.86d1effd4c412b85ac13db53a90c473a0f256f789b821e131125c9aa25cb6a6d.css integrity="sha256-htHv/UxBK4WsE9tTqQxHOg8lb3ibgh4TESXJqiXLam0="><link type=text/css rel=stylesheet href=https://uther.wtf/css/custom.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css integrity="sha256-47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU="><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/uther.wtf"},"articleSection":"blog","name":"Swarming Spark","headline":"Swarming Spark","description":"","inLanguage":"en-US","author":"","creator":"","publisher":"","accountablePerson":"","copyrightHolder":"","copyrightYear":"2015","datePublished":"2015-04-30 00:00:00 \u002b0000 UTC","dateModified":"2015-04-30 00:00:00 \u002b0000 UTC","url":"https:\/\/uther.wtf\/blog\/swarming-spark\/","wordCount":"997","keywords":["Blog"]}</script></head><body><div class=burger__container><div class=burger aria-controls=navigation aria-label=Menu><div class="burger__meat burger__meat--1"></div><div class="burger__meat burger__meat--2"></div><div class="burger__meat burger__meat--3"></div></div></div><nav class=nav id=navigation><ul class=nav__list><li><a href=https://uther.wtf/>Home</a></li><li><a href=https://uther.wtf/cv>Curriculum vitae</a></li><li><a class=active href=https://uther.wtf/blog>blog</a></li></ul></nav><main><div class=flex-wrapper><div class=post__container><div class=post><header class=post__header><h1 id=post__title>Swarming Spark</h1><time datetime="2015-04-30 00:00:00 +0000 UTC" class=post__date>2015-04-30</time></header><article class=post__content><p><a href=https://spark.apache.org/ target=_blank rel="noreferrer noopener">Spark</a> is a useful bunch of stuff for processing large amounts of data, offering a friendly and fast functional interface over map-reduce on a cluster of machines, with some extra bits like cacheable datasets. It’s relatively easy to get running too (although with a list of gotchas), with scripts to start a stand-alone cluster on EC2, or pretty simple tutorials for running on mesos, and their deployment guides give a good overview of the raw process.</p><p>But there are a bunch of new cluster management services popping up that rely on <a href=https://www.docker.com/ target=_blank rel="noreferrer noopener">docker</a>: <a href=http://aws.amazon.com/ecs/ target=_blank rel="noreferrer noopener">ECS</a> (on EC2), <a href=https://cloud.google.com/container-engine/ target=_blank rel="noreferrer noopener">GCE</a> (on google), <a href=http://mesos.apache.org/ target=_blank rel="noreferrer noopener">Mesos</a> now takes docker containers, and Azure. Elastic beanstalk had some support. Docker is clearly the new hotness for packaging something like Spark, but the Docker container does not contain enough information on it’s own to get a cluster system like Spark running. For that you need to allow the workers to discover the master, allow the driver to access the master from outside, replicate workers, etc, etc. All this can be done, but there’s no standard for it. Each container cluster manager has a slightly different approach. You can find docker containers for Spark, but they tend not to be extremely portable which seems a bit broken.</p><p>So the itch I want to scratch is ‘Can we build docker containers for Spark that easily run on more than one cluster manager, say GCE (which implements <a href=http://kubernetes.io/ target=_blank rel="noreferrer noopener">kubernetes</a>) and ECS?. Also, how good are the cross-provider service layers like kubernetes and [swarm](<a href=https://docs.docker.com/swarm/ target=_blank rel="noreferrer noopener">https://docs.docker.com/swarm/</a>?’ There seem to be 3 levels of difficulty here. The first is getting the master and workers all running. The second is getting them to find and talk to each other. The third is allowing a driver (which in spark is also part of the akka cluster) to talk to them all.</p><p>Getting them running is easy. Spark comes with sensible shell scripts to start each, and wrapping them in docker is no problem. For Kubernetes (GCS), we can just run the master as a pod, and the workers under a replication controller (and hence N pods). For ECS the story is almost identical, and the json configuration even looks very similar.</p><p>Getting them to talk to each other is a bit more tricky. Background: Docker doesn’t come with any built-in discovery mechanisms. You can link containers together on a single host, but not across multiple hosts (without external help). They used to suggest the ambassador pattern for this, but now docker is working on swarm. Kubernetes allows you to describe ‘services’ which can be found via environment variables or optionally DNS. <a href=https://github.com/coreos/flannel target=_blank rel="noreferrer noopener">Flannel</a> tries to follow kubernetes. <a href=http://weave.works/ target=_blank rel="noreferrer noopener">Weave</a> kind of sits around docker and creates a complete network around containers, but is not what GCE or ECS use natively, so I’ll ignore it here.</p><p>So, once we have the master up, it has an address. In Kubernetes we can create a service to expose the master as a fixed entity (binding to it based on name), and then the workers can know it’s IP address and port through environment variables, so you need to make sure you start the worker in a shell so it can do the variable substitution. But here we run into a little difficulty in that Akka is picky about what hostname actors are bound to. So it’s actually best to configure the workers to find the master through <code>KUBE_DNS</code>, which our service has exposed as spark-master, so the startup script actually needs to configure <code>/etc/resolv.conf</code>. ECS doesn’t by default support anything outside docker so this would require manual labour – I suppose for that we would have to implement the ambassador pattern for this?. This is looking less easy!</p><p>But there’s a light at the end of the tunnel here. Kubernetes will also run on EC2 (bypassing ECS), so we can take the same configuration we used on GCE and spin it up on Amazon! Well, with the small wrinkle that the external address of our master service is just the public IP of the host host the container is running on, rather than one allocated to the service itself as in GCE. Indeed if you add ‘createExternalLoadBalancer’ to our service descriptor ECS complains, so our master service json file needs to be slightly different between GCE and ECS.</p><p>What about Swarm? It’s a bit of a newbie here, but it promises to extend the <code>–link</code> mechanism of docker across hosts that are clustered in a swarm (perhaps started with <code>docker-machine –swarm</code>). And guess what, it does! In fact, this was the easiest way to get things going. I can even get rid of the <code>resolv.conf</code> hack. Here’s how:</p><p>Docker-compose works nicely to configure how our application will be structured, so a short <code>docker-compose.yaml</code> file shows the master and how it’s used by the worker:</p><pre><code class=language-Dockerfile>master:
image: snufkin/spark-master
hostname: spark-master
ports:
- &quot;8080&quot;
- &quot;4040&quot;
- &quot;7077:7077&quot;
expose:
- &quot;7077&quot;
worker:
image: snufkin/spark-worker
ports:
- &quot;8081&quot;
links:
- master:spark-master
</code></pre><p>We use <code>docker-machine</code> to start our machines on google compute engine or EC2. A simple shell script does for this.</p><pre><code class=language-bash>docker-machine create -d virtualbox local
eval &quot;$(docker-machine env local)&quot;
export SWARMID=`docker run swarm create`
echo $SWARMID
docker-machine create -d amazonec2 --amazonec2-access-key $AWSKEY --amazonec2-region $REGION --amazonec2-secret-key $AWSSECKEY --amazonec2-vpc-id $VPCID --swarm --swarm-master --swarm-discovery token://$SWARMID swarm-master
# would be nice to use gnu parallel here, but fails when faced with the google oauth calls if you're using the google driver
for i in $(seq 1 $NUMNODES); do
docker-machine create -d amazonec2 --amazonec2-access-key $AWSKEY --amazonec2-region $REGION --amazonec2-secret-key $AWSSECKEY --amazonec2-vpc-id $VPCID --swarm --swarm-discovery token://$SWARMID swarm-node-$i;
done;
eval $(docker-machine env --swarm swarm-master)
docker info
docker-compose up -d
docker-compose scale worker=3
docker-compose logs
docker ps
</code></pre><p>Then find out the public IP of the host the master is running on, open port 7077 to said host (security groups or network config on the cloud provider), add the host to your <code>/etc/hosts</code> as spark-master (to satisfy Akka, which is picky about hostnames), and try something like</p><pre><code class=language-bash>spark$ ./bin/spark-shell --master spark://spark-master:7077
</code></pre><p>See, easy!</p><p>(<a href=http://web.archive.org/web/20161006223524/http://www.lshift.net/blog/2015/04/30/swarming-spark/ target=_blank rel="noreferrer noopener">http://web.archive.org/web/20161006223524/http://www.lshift.net/blog/2015/04/30/swarming-spark/</a>)</p></article><div class=pagination><a class=pagination__item href=https://uther.wtf/blog/lets-run-science/><span class=pagination__label>Previous Post</span>
<span class=pagination__title>Lets Run Science</span></a>
<a class=pagination__item href=https://uther.wtf/blog/on-being-almost-there/><span class=pagination__label>Next Post</span>
<span class=pagination__title>On Being Almost There</span></a></div><footer class=post__footer><div class=social-icons><a class=social-icons__link title=Twitter href=https://twitter.com/hemul target=_blank rel="me noopener"><div class=social-icons__icon style=background-image:url(https://uther.wtf/svg/twitter.svg)></div></a><a class=social-icons__link title=GitHub href=https://github.com/jbu target=_blank rel="me noopener"><div class=social-icons__icon style=background-image:url(https://uther.wtf/svg/github.svg)></div></a><a class=social-icons__link title=LinkedIn href=https://www.linkedin.com/in/jamesuther/ target=_blank rel="me noopener"><div class=social-icons__icon style=background-image:url(https://uther.wtf/svg/linkedin.svg)></div></a></div><p>© 2022</p></footer></div></div></div></main><script src=https://uther.wtf/js/index.min.301a8b0870381bf76b3b5182e8966d363a0474281183439beb024d8b8228fc66.js integrity="sha256-MBqLCHA4G/drO1GC6JZtNjoEdCgRg0Ob6wJNi4Io/GY=" crossorigin=anonymous></script>
<script src=https://unpkg.com/prismjs@1.20.0/components/prism-core.min.js></script>
<script src=https://unpkg.com/prismjs@1.20.0/plugins/autoloader/prism-autoloader.min.js data-autoloader-path=https://unpkg.com/prismjs@1.20.0/components/></script></body></html>