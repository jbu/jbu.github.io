<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="utf-8">
      <link rel="stylesheet" href="https://latex.now.sh/style.css" />
      <!-- <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> -->
      <link rel="stylesheet" href="https://latex.now.sh/prism/prism.css">
      <script src="https://cdn.jsdelivr.net/npm/prismjs/prism.min.js"></script>
   </head>

   <body class="libertinus latex-dark-auto">

      <h1>
         Lunchtime Hack - Lets Run Science
      </h1>
      <p class="author">James Uther <br>2015-02-27</p>
      <div class="abstract">

         Possibly part 1.

      </div>
      <p>
         Who else likes visiting science museums? All those old apparatus &dash;
         bits of the radio telescope that first saw pulsars, longitude prize
         clocks, jury-rigged ingenious devices that captured the first glimpse
         of something new and exciting. One day the LHC will be dismantled,
         carved up and shipped to museums around the world. For a lunchtime hack
         I'm going to go and have a look at something along those lines, but
         software.
      </p><p>
         Climate records have been collected in one form or another for hundreds
         of years (<a
            href="http://www.realclimate.org/index.php/data-sources/#Climate_data_raw)">here
            are some</a>
         and we can get proxies going back <a
            href="http://www.ncdc.noaa.gov/data-access/paleoclimatology-data/datasets">way
            further</a>. Collecting this data has been the sort of heroic
         grunt-work of real science.
      </p><p>
         But if you thought the sales data you're trying to analize is a mess,
         you lead a sheltered life! One of the apparatus that has really made a
         scientific and social impact over the last few decades has been
         methods, implemented in code, for pulling these data sets into
         something that can show whether or not the earth's temperature is
         changing, and with a bit more work, why. So let's go and visit one of
         these devices and take it for a spin.
      </p><p>
         There are a few of these things to choose from. <a
            href="http://berkeleyearth.org/land-and-ocean-data">Berkeley
            Earth</a> is a recent one
         but it's all in matlab which I don't have installed, so move on.
         <a href="http://data.giss.nasa.gov/gistemp/">GISTEMP</a> from NASA is
         another major look at surface temperature.
         Ahh, it has downloadable code and data that looks mostly python and a
         bit of fortran. Let's see what we can get running.
      </p><p>
         I'm expecting (and found) bitrot. Docker seems like it would be a
         useful way of preserving these sorts of projects, because it can manage
         the runtime dependencies, so I'm going to build a docker image of my
         work (and a <code class="language-bash">run.sh</code>) as I go. You can
         get it <a
            href="https://github.com/jbu/gistemp-docker). Build this container,
            run it (and be dropped into the shell">here</a> with <code
            class="language-bash">./build.sh</code>.
         Then run <code class="language-bash">./gistemp.sh</code>. You'll need
         `docker` and <code class="language-bash">docker-machine</code>.
      </p><p>
         So, the readmees are pretty clear. Some data needs to be fetched and
         some of the URLs have changed slightly but it's easy enough to track
         down. It takes in data from GHCNv3 (Global Historical Climate Network
         from NOAA) and Antarctic SCAR (Scientific Committee on Antarctic
         Research) data.
      </p><p>
         Step 0, which merges the sources:

         <pre>
   <code class="language-bash">
$ do_comb_step0.sh
reformatting to v2 format
Bringing Antarctic tables closer to v2.mean format
collecting surface station data
... and autom. weather stn data
... and australian data
replacing '-' by -999.9, blanks are left alone at this stage
adding extra Antarctica station data to input_files/v3.mean
created v2.meanx from v2_antarct.dat and input_files/v3.mean

GHCN data:
 removing data before year   1880.00000
created v2.meanz from v2.meanx
replacing Hohenspeissenberg data in v3.mean by more complete data (priv.comm.)
disregard pre-1880 data:

created v3.mean_comb
move files from temp_files/. to ../STEP1/temp_files/.
and execute in the STEP1 directory the command:
   do_comb_step1.sh v3.mean_comb
</code></pre>
      </p><p>
         Now, Step 1 'eliminates some dubious records'. It has some python
         extensions that need to be compiled &dash; In fact I end up patching it
         slightly and moving to <code class="language-bash">setup.py</code>. But
         then

         <pre>
   <code class="language-bash">
Creating v3.mean_comb.bdb
reading v3.mean_comb
reading v3.inv
writing v3.mean_comb.bdb
Dropping strange data
reading Ts.strange.RSU.list.IN
reading v3.mean_comb.bdb
writing v3.mean_comb.strange.bdb
reading v3.mean_comb.strange.bdb
creating v3.mean_comb.strange.txt
1000
2000
...
created Ts.txt
move this file from STEP1/temp_files to STEP2/temp_files
and execute in the STEP2 directory the command:
   do_comb_step2.sh last_year_with_data
</code></pre>
      </p><p>
         Step 2: Splitting into zonal sections and homogenization. This compares
         rural and urban stations to remove any effect of the urban environment.
         <pre>
   <code class="language-bash">
converting text to binary file
 last year with data:        2013  LightGl=           1
        1000 processed so far
        2000 processed so far
        3000 processed so far
        4000 processed so far
        5000 processed so far
        6000 processed so far
        7000 processed so far
 number of station ids:        7354
 ...
 created Ts.GHCN.CL.* files
move them from STEP2/temp_files to STEP3/temp_files
and execute in STEP3 do_comb_step3.sh
</code></pre>

         and then
      </p><p>
         Step 3 : Gridding and computation of zonal means

         ...
      </p><p>
         gives us a file <code class="language-bash">SBBX1880.Ts.GHCN.CL.PA.1200</code>
         which is a nasty fortran
         blob that apparently contains the grid of surface temperature
         anomalies. Great! Let's plot that! There are some files at nasa (linked
         in the build/run scripts here) that can convert this <code
            class="language-bash">SBBX</code> file to a
         <code class="language-bash">NetCDF</code> file that can be viewed.
         Problem is, the programs supplied
         take some memory. In fact, a few random checks suggest 10s of Gigs,
         which is beyond my macbook. So let's leave it hereâ€¦ But what we were
         shooting for was
      </p><p>
         <figure>
            <img
               src="../static/lets-run-science.gif"
               loading="lazy" alt="Temperature anomaly 1951-1980" width="600"
               height="400">
            <figcaption>
               Temperature anomaly 1951-1980
            </figcaption>
         </figure>
      </p><p>
         So there you are. Just like repeating a cloud-chamber experiment in
         school physics we haven't actually learned anything that hasn't already
         been published and pored over. We haven't actually looked at the code
         to really see what it's doing.
      </p><p>
         But, I now have a major bit of science history (from NASA!) running
         (mostly) on my laptop. That's at least a bit cool.
      </p><p>
         Also, docker has more uses than you think.

      </p><p>

         (Originally
         <a href="http://web.archive.org/web/20161006160845/http://www.lshift.net/blog/2015/02/27/lunchtime-hack-lets-run-science/">
            http://web.archive.org/web/20161006160845/http://www.lshift.net/blog/2015/02/27/lunchtime-hack-lets-run-science/</a>)
      </p></body></html>